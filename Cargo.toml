[package]
name = "llm-bench"
version = "0.1.0"
edition = "2024"
authors = ["Brian Martin <brayniac@gmail.com>"]
description = "A benchmarking tool for OpenAI-compatible LLM servers"
license = "MIT OR Apache-2.0"

[dependencies]
anyhow = "1.0"
chrono = { version = "0.4", features = ["serde"] }
clap = { version = "4.4", features = ["derive"] }
futures = "0.3"
humantime = "2.1"
log = "0.4"
metriken = "0.7"
metriken-exposition = { version = "0.12.2", features = ["json", "msgpack"] }
num_cpus = "1.16"
rand = "0.8"
rand_distr = "0.4"
reqwest = { version = "0.11", features = ["json", "stream"] }
ringlog = "0.8"
rmp-serde = "1.3"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tempfile = "3.8"
thiserror = "1.0"
tiktoken-rs = "0.6"
tokio = { version = "1.35", features = ["full"] }
tokio-stream = "0.1"
toml = "0.8"
warp = "0.3"

[dev-dependencies]
mockito = "1.2"
pretty_assertions = "1.4"
